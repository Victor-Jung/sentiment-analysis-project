{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Imports ###############\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from functions import clean\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Preprocessing ###\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "### Tensorflow ###\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "### API ###\n",
    "import requests\n",
    "import base64\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetSize = 1600000\n",
    "trainingPart = 0.8\n",
    "testingPart =  0.2\n",
    "split = int(datasetSize*trainingPart)\n",
    "tokenizer = None\n",
    "\n",
    "embedding_dim = 100    # glove6b100\n",
    "max_length = 20        # max lenght of a tweet\n",
    "trunc_type='post'      # it will cut the tweet if it is longer than 20\n",
    "padding_type='post'    # it will add zeros at the end the tweets if is smaller than 20\n",
    "oov_tok = \"<OOV>\"      # for unseen words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset():\n",
    "    if Path(\"./data/cleaned_sentiment.csv\").is_file():\n",
    "        df = pd.read_csv(\"./data/cleaned_sentiment.csv\", encoding='latin')\n",
    "        df = df.replace(np.nan, '', regex=True)\n",
    "    else:\n",
    "        df = pd.read_csv(\"./data/training.1600000.processed.noemoticon.csv\", encoding='latin')\n",
    "        df.columns = ['Label','Id','Date','Query','Name','Text']\n",
    "        df = df.drop(columns=['Date', 'Query', 'Name', 'Id'])\n",
    "        df['Label'] = df['Label'].replace(4, 1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    \n",
    "    if not Path(\"./data/cleaned_sentiment.csv\").is_file():\n",
    "        tokenizer = Tokenizer()\n",
    "        rawData = df['Text'].to_numpy()\n",
    "        rawLabels = df['Label'].to_numpy()\n",
    "        tokenizer = TweetTokenizer(strip_handles=True)\n",
    "        selectedData = []\n",
    "        corpus = []\n",
    "\n",
    "        print(\"Replacing Contractions and Clean Data :\")\n",
    "        with tqdm(total=datasetSize*1.5) as pbar: # The dataset is sorted by labels so we make sure to get our 2 labels in our subset\n",
    "            for i in range(int(datasetSize/2)):\n",
    "                selectedData.append([rawData[i], rawLabels[i]])\n",
    "                selectedData.append([rawData[len(rawData)-i-1], rawLabels[len(rawData)-i-1]])\n",
    "                pbar.update(1)\n",
    "            for i in range(datasetSize):\n",
    "                corpus.append([clean(selectedData[i][0]), selectedData[i][1]])\n",
    "                pbar.update(1)\n",
    "        cleaned_df = pd.DataFrame(corpus, columns =['Text', 'Label'])\n",
    "        cleaned_df.to_csv('./data/cleaned_sentiment.csv', index=True)\n",
    "    else:\n",
    "        rawData = df['Text'].to_numpy()\n",
    "        rawLabels = df['Label'].to_numpy()\n",
    "        selectedData = []\n",
    "        corpus = []\n",
    "        \n",
    "        for elt in rawData:\n",
    "            if not isinstance(elt, str):\n",
    "                print(elt)\n",
    "        \n",
    "        for i in range(int(datasetSize/2)):\n",
    "            selectedData.append([rawData[i], rawLabels[i]])\n",
    "            selectedData.append([rawData[len(rawData)-i-1], rawLabels[len(rawData)-i-1]])\n",
    "        for i in range(datasetSize):\n",
    "            corpus.append([selectedData[i][0], selectedData[i][1]])\n",
    "                \n",
    "    return corpus\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padded_sequences(corpus):\n",
    "    \n",
    "    sentences=[]\n",
    "    labels=[]\n",
    "    \n",
    "    random.shuffle(corpus)\n",
    "\n",
    "    for x in range(datasetSize):\n",
    "        sentences.append(corpus[x][0])\n",
    "        labels.append(corpus[x][1])\n",
    "        \n",
    "    # Using tf tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size=len(word_index)\n",
    "\n",
    "    # Passing with tensorflow tools\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "    \n",
    "    return padded, labels, tokenizer, word_index, vocab_size\n",
    "\n",
    "def split_dataset(padded, labels, split):\n",
    "    \n",
    "    test_sequences = padded[split:len(padded)]\n",
    "    test_labels = labels[split:len(labels)]\n",
    "\n",
    "    training_sequences = padded[0:split]\n",
    "    training_labels = labels[0:split]\n",
    "    \n",
    "    return training_sequences, training_labels, test_sequences, test_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import GloVe 100 Dimensions Embedding sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_matrix():\n",
    "    \n",
    "    embeddings_index = {};\n",
    "\n",
    "    with open('./data/glove.6B.100d.txt') as f:\n",
    "        for line in f:\n",
    "            values = line.split();\n",
    "            word = values[0];\n",
    "            coefs = np.asarray(values[1:], dtype='float32');\n",
    "            embeddings_index[word] = coefs;\n",
    "\n",
    "    embeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word);\n",
    "        if embedding_vector is not None:\n",
    "            embeddings_matrix[i] = embedding_vector;\n",
    "            \n",
    "    return embeddings_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline_model(training_sequences, training_labels, test_sequences, test_labels, embeddings_matrix, vocab_size, num_epochs):\n",
    "\n",
    "    baseline_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n",
    "            tf.keras.layers.SimpleRNN(128),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    if Path(\"./data/baseline_weights.h5\").is_file():\n",
    "        baseline_model.load_weights(\"./data/baseline_weights.h5\")\n",
    "\n",
    "    baseline_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    baseline_model.summary()\n",
    "\n",
    "    training_padded = np.array(training_sequences)\n",
    "    training_labels = np.array(training_labels)\n",
    "    testing_padded = np.array(test_sequences)\n",
    "    testing_labels = np.array(test_labels)\n",
    "\n",
    "    logs_base_dir = \"./logs\"\n",
    "    \n",
    "    os.makedirs(logs_base_dir, exist_ok=True)\n",
    "    logdir = os.path.join(logs_base_dir, \"baseline-\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "    baseline_history = baseline_model.fit(training_padded, \n",
    "                        training_labels, \n",
    "                        epochs=num_epochs, \n",
    "                        validation_data=(testing_padded, testing_labels),\n",
    "                        batch_size = 512,\n",
    "                        verbose=1,\n",
    "                        callbacks=[tensorboard_callback])\n",
    "    \n",
    "    print(\"Training Complete\")\n",
    "    \n",
    "    baseline_model.save_weights(\"./data/baseline_weights.h5\", True)\n",
    "    \n",
    "    return baseline_model, baseline_history\n",
    "\n",
    "def train_model(training_sequences, training_labels, test_sequences, test_labels, embeddings_matrix, vocab_size, num_epochs):\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Bidirectional(LSTM(units=64, return_sequences=True)),\n",
    "            tf.keras.layers.Bidirectional(LSTM(units=128)),\n",
    "            tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    if Path(\"./data/myWeights.h5\").is_file():\n",
    "        model.load_weights(\"./data/myWeights.h5\")\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    training_padded = np.array(training_sequences)\n",
    "    training_labels = np.array(training_labels)\n",
    "    testing_padded = np.array(test_sequences)\n",
    "    testing_labels = np.array(test_labels)\n",
    "\n",
    "    logs_base_dir = \"./logs\"\n",
    "    os.makedirs(logs_base_dir, exist_ok=True)\n",
    "    logdir = os.path.join(logs_base_dir, \"LSTM-\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "    history = model.fit(training_padded, \n",
    "                        training_labels, \n",
    "                        epochs=num_epochs, \n",
    "                        validation_data=(testing_padded, testing_labels),\n",
    "                        batch_size = 512,\n",
    "                        verbose=1,\n",
    "                        callbacks=[tensorboard_callback])\n",
    "    print(\"Training Complete\")\n",
    "    \n",
    "    model.save_weights(\"./data/myWeights.h5\", True)\n",
    "    \n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 20, 100)           41577900  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 20, 128)           84480     \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 41,942,061\n",
      "Trainable params: 364,161\n",
      "Non-trainable params: 41,577,900\n",
      "_________________________________________________________________\n",
      "   1/2500 [..............................] - ETA: 3s - loss: 0.5221 - accuracy: 0.7344WARNING:tensorflow:From /home/victor/.conda/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "   2/2500 [..............................] - ETA: 1:23 - loss: 0.4983 - accuracy: 0.7422WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0086s vs `on_train_batch_end` time: 0.0561s). Check your callbacks.\n",
      "2500/2500 [==============================] - 65s 26ms/step - loss: 0.4956 - accuracy: 0.7528 - val_loss: 0.4671 - val_accuracy: 0.7732\n",
      "Training Complete\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 20, 100)           41577900  \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 128)               29312     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 41,615,533\n",
      "Trainable params: 37,633\n",
      "Non-trainable params: 41,577,900\n",
      "_________________________________________________________________\n",
      "   2/2500 [..............................] - ETA: 2:06 - loss: 0.5335 - accuracy: 0.7373WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0099s vs `on_train_batch_end` time: 0.0901s). Check your callbacks.\n",
      "2500/2500 [==============================] - 16s 7ms/step - loss: 0.5054 - accuracy: 0.7482 - val_loss: 0.4982 - val_accuracy: 0.7533\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "df = read_dataset()\n",
    "corpus = clean_dataset(df)\n",
    "padded, labels, tokenizer, word_index, vocab_size = create_padded_sequences(corpus)\n",
    "training_sequences, training_labels, test_sequences, test_labels  = split_dataset(padded, labels, split)\n",
    "embeddings_matrix = create_embeddings_matrix()\n",
    "model, history = train_model(training_sequences, training_labels, test_sequences, test_labels, embeddings_matrix, vocab_size, 1)\n",
    "baseline_model, baseline_history = train_baseline_model(training_sequences, training_labels, test_sequences, test_labels, embeddings_matrix, vocab_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer():\n",
    "    df = read_dataset()\n",
    "    corpus = clean_dataset(df)\n",
    "    padded, labels, tokenizer, word_index, vocab_size = create_padded_sequences(corpus)\n",
    "    return tokenizer, vocab_size, word_index\n",
    "\n",
    "def pipeline(s, tokenizer, vocab_size):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Bidirectional(LSTM(units=64, return_sequences=True)),\n",
    "            tf.keras.layers.Bidirectional(LSTM(units=128)),\n",
    "            tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    if Path(\"./data/myWeights.h5\").is_file():\n",
    "        model.load_weights(\"./data/myWeights.h5\")\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    sentence = clean(s)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences([sentence])\n",
    "    padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "    padded = np.asarray(padded)\n",
    "    \n",
    "    val = model.predict(padded)[0][0]\n",
    "    res = \"\"\n",
    "    \n",
    "    if val > 0.9:\n",
    "        res = \"Very Positive\"\n",
    "    elif val > 0.7:\n",
    "        res = \"Positive\"\n",
    "    elif val < 0.1:\n",
    "        res = \"Very Negative\"\n",
    "    elif val < 0.3:\n",
    "        res = \"Negative\"\n",
    "    else:\n",
    "        res = \"Neutral\"\n",
    "\n",
    "    print(s, \":\", res, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You were the chosen one, you were supposed to destroy siths, not join them ! : Neutral 0.6172293\n",
      "I hate you !!! : Very Negative 0.073474854\n",
      "You were my brother Anakin, I loved you : Positive 0.7988903\n"
     ]
    }
   ],
   "source": [
    "### Testing the Pipeline ###\n",
    "\n",
    "if tokenizer is None:\n",
    "    tokenizer, vocab_size, word_index = create_tokenizer()\n",
    "    embeddings_matrix = create_embeddings_matrix()\n",
    "\n",
    "pipeline(\"You were the chosen one, you were supposed to destroy siths, not join them !\", tokenizer, vocab_size)\n",
    "pipeline(\"I hate you !!!\", tokenizer, vocab_size)\n",
    "pipeline(\"You were my brother Anakin, I loved you\", tokenizer, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Pipeline With API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@keithtaylor21 Anthony Epps was the best! Iâ€™m from Marion County so Iâ€™m biased! : Positive 0.7533767\n",
      "None\n",
      "Out of all this I still be feeling alone sometimes : Very Negative 0.042525433\n",
      "None\n",
      "@blueheartedly https://t.co/Ipvojw76Iy : Neutral 0.54378545\n",
      "None\n",
      "We all need this reminder https://t.co/pS6UdihmxR : Neutral 0.3880049\n",
      "None\n",
      "My buddy! https://t.co/Uo1frewITE : Neutral 0.44178596\n",
      "None\n",
      "I think I turned into @experiment_719!!! I have a lot of Starbucks cups coming in the mail soon. ðŸ’šâœ¨ : Neutral 0.47046798\n",
      "None\n",
      "deter and maybe apprehend some of these miscreants. Some operators have bad attitudes and need to check that causeâ€¦ https://t.co/iAKVlPepDH : Neutral 0.3751005\n",
      "None\n",
      "@BriannamThe Coming from you, gorgeous ðŸ¥° : Very Positive 0.9610776\n",
      "None\n",
      "Thank you @Cook4Rep for not placing politics before MO kids and MO public schools. https://t.co/zMZR2Intkw : Very Positive 0.98375225\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "api_key = 'ckyDjZhsNFL71ATy2GxinMXKA'\n",
    "api_secret_key = 'SURpuD1Pwwytmj6bBTj9zWXMrRNUVTUuAGPadc5UTGMOaGshej'\n",
    "access_token = '2830156159-WacIqOzbncMAJX8tzKKZfytAdPAVqFbbiihbWht'\n",
    "access_secret_token = 'u5YIhre5YWLHe0f63iKgb0TTHljDaqnyzC5QEVF0zQvWf'\n",
    "\n",
    "auth = tweepy.OAuthHandler(api_key, api_secret_key)\n",
    "auth.set_access_token(access_token, access_secret_token)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "places = api.geo_search(query=\"USA\", granularity=\"country\")\n",
    "place_id = places[0].id\n",
    "\n",
    "tweets = api.search(q=\"place:%s\" % place_id)\n",
    "\n",
    "if tokenizer is None:\n",
    "    tokenizer, vocab_size, word_index = create_tokenizer()\n",
    "    embeddings_matrix = create_embeddings_matrix()\n",
    "    \n",
    "for tweet in tweets:\n",
    "    print(pipeline(tweet.text, tokenizer, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6bce0edfeda871ea\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6bce0edfeda871ea\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir \"./logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
